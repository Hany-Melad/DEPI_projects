{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b081d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9ceeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e3a945",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/annotations.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[39m, in \u001b[36m_open_file\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'path/to/annotations.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# استخدام:\u001b[39;00m\n\u001b[32m     41\u001b[39m transform = transforms.Compose([\n\u001b[32m     42\u001b[39m     transforms.Resize((\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)),\n\u001b[32m     43\u001b[39m     transforms.ToTensor(),\n\u001b[32m     44\u001b[39m     transforms.Normalize(mean=[\u001b[32m0.485\u001b[39m, \u001b[32m0.456\u001b[39m, \u001b[32m0.406\u001b[39m], \n\u001b[32m     45\u001b[39m                         std=[\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m])\n\u001b[32m     46\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m dataset = \u001b[43mMPIIFaceGazeDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpath/to/annotations.mat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpath/to/images\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m loader = DataLoader(dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMPIIFaceGazeDataset.__init__\u001b[39m\u001b[34m(self, mat_file_path, image_dir, transform)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mat_file_path, image_dir, transform=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     10\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    mat_file_path: مسار ملف التعليقات (.mat)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m    image_dir: مجلد الصور الأصلية\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[43msio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m.image_dir = image_dir\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m.transform = transform\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:234\u001b[39m, in \u001b[36mloadmat\u001b[39m\u001b[34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03mLoad MATLAB file.\u001b[39;00m\n\u001b[32m     90\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    231\u001b[39m \u001b[33;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    233\u001b[39m variable_names = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mvariable_names\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat_reader_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatfile_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[39m, in \u001b[36m_open_file_context\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_context\u001b[39m(file_like, appendmat, mode=\u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     f, opened = \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:45\u001b[39m, in \u001b[36m_open_file\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like.endswith(\u001b[33m'\u001b[39m\u001b[33m.mat\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     44\u001b[39m         file_like += \u001b[33m'\u001b[39m\u001b[33m.mat\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m     48\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mReader needs file name or open file-like object\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     49\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'path/to/annotations.mat'"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MPIIFaceGazeDataset(Dataset):\n",
    "    def __init__(self, mat_file_path, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        mat_file_path: مسار ملف التعليقات (.mat)\n",
    "        image_dir: مجلد الصور الأصلية\n",
    "        \"\"\"\n",
    "        self.data = sio.loadmat(mat_file_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # استخراج البيانات\n",
    "        self.images = self.data['Data']['image'][0, 0]  # مسارات الصور\n",
    "        self.gaze = self.data['Data']['gaze'][0, 0]      # متجهات النظر (3D)\n",
    "        self.head_pose = self.data['Data']['pose'][0, 0] # وضعية الرأس\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # قراءة الصورة\n",
    "        img_path = f\"{self.image_dir}/{self.images[idx][0]}\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # استخراج labels\n",
    "        gaze_label = torch.tensor(self.gaze[idx], dtype=torch.float32)\n",
    "        head_label = torch.tensor(self.head_pose[idx], dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, gaze_label, head_label\n",
    "\n",
    "# استخدام:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = MPIIFaceGazeDataset('path/to/annotations.mat', 'path/to/images', transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b69675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install scipy\n",
    "#! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9fff8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\DELL/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m model = GazeEstimationModel()\n\u001b[32m     36\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m1e-4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, gaze_gt, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloader\u001b[49m:\n\u001b[32m     39\u001b[39m     optimizer.zero_grad()\n\u001b[32m     40\u001b[39m     gaze_pred = model(images)\n",
      "\u001b[31mNameError\u001b[39m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class GazeEstimationModel(nn.Module):\n",
    "    def __init__(self, output_dim=3):\n",
    "        super(GazeEstimationModel, self).__init__()\n",
    "        # استخدام ResNet18 كـ Backbone\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # تعديل الطبقة الأخيرة لإخراج متجه 3D (اتجاه النظر)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, output_dim)  # x, y, z للنظر\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gaze = self.backbone(x)\n",
    "        # تسوية المتجه ليكون وحدة طول (Unit Vector)\n",
    "        gaze = F.normalize(gaze, p=2, dim=1)\n",
    "        return gaze\n",
    "\n",
    "# دوال التدريب:\n",
    "def angular_loss(pred, target):\n",
    "    \"\"\"حساب الخطAngular بين متجهين\"\"\"\n",
    "    cos_sim = F.cosine_similarity(pred, target)\n",
    "    # تحويل cos similarity إلى زاوية بالراديان\n",
    "    angle = torch.acos(torch.clamp(cos_sim, -1.0, 1.0))\n",
    "    return torch.mean(angle)\n",
    "\n",
    "# مثال على loop تدريب:\n",
    "model = GazeEstimationModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for images, gaze_gt, _ in loader:\n",
    "    optimizer.zero_grad()\n",
    "    gaze_pred = model(images)\n",
    "    loss = angular_loss(gaze_pred, gaze_gt)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de94447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gaze_model(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    total_error = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, gaze_gt, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            gaze_gt = gaze_gt.to(device)\n",
    "            \n",
    "            gaze_pred = model(images)\n",
    "            \n",
    "            # حساب الخط الزاوي بالدرجات\n",
    "            cos_sim = F.cosine_similarity(gaze_pred, gaze_gt)\n",
    "            angles = torch.acos(torch.clamp(cos_sim, -1.0, 1.0)) * (180.0 / np.pi)\n",
    "            \n",
    "            total_error += torch.sum(angles).item()\n",
    "            count += len(angles)\n",
    "    \n",
    "    mean_error = total_error / count\n",
    "    print(f\"Mean Angular Error: {mean_error:.2f} degrees\")\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b06339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
